{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df78ada1",
   "metadata": {},
   "source": [
    "## Tut1-2NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e1c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statistics import mean, pstdev\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0e8c5",
   "metadata": {},
   "source": [
    "Reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c92eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 2025):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)                    # Fix NumPy random seed\n",
    "    torch.manual_seed(seed)                 # Fix PyTorch random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2b712",
   "metadata": {},
   "source": [
    "2NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82334fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.out = nn.Linear(200, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b0c20",
   "metadata": {},
   "source": [
    "IID, where the data is shuffled, and then partitioned into 100 clients each receiving 600 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac51048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_iid_parts(train_ds, K=100):\n",
    "    idx = np.arange(len(train_ds))              # Get all sample indices\n",
    "    np.random.shuffle(idx)                      # Shuffle globally (ensures IID)\n",
    "    parts = np.array_split(idx, K)              # Split into K parts evenly\n",
    "    return [p.astype(int) for p in parts]       # Convert to int, needed by Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1824daf",
   "metadata": {},
   "source": [
    "Non-IID,\n",
    "where we first sort the data by digit label, divide it into 200 shards of size 300, and assign each of 100 clients 2 shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a1cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pathological_noniid_parts(train_ds, K=100):\n",
    "    n = len(train_ds)                           # MNIST training set has 60,000 samples\n",
    "    # Extract labels and create (idx, label) mapping key\n",
    "    labels = np.fromiter((train_ds[i][1] for i in range(n)), dtype=np.int64, count=n)\n",
    "    idx_sorted = np.argsort(labels)             # Sort indices by label (so each shard is almost single-class)\n",
    "\n",
    "    shards = 2 * K                              # Paper setting: 200 shards\n",
    "    shard_size = n // shards                    # Each shard has 300 samples\n",
    "    shard_list = [idx_sorted[i*shard_size:(i+1)*shard_size] for i in range(shards)]\n",
    "    np.random.shuffle(shard_list)               # Shuffle shard order to avoid bias\n",
    "\n",
    "    parts = []\n",
    "    for k in range(K):\n",
    "        # Each client gets 2 shards (so most clients only see 2 digit classes → non-IID)\n",
    "        part = np.concatenate([shard_list[2*k], shard_list[2*k + 1]])\n",
    "        parts.append(part.astype(int))\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8ec62",
   "metadata": {},
   "source": [
    "ClientUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb13e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update(global_model, dataset, indices, E=1, B=10, lr=0.1, device=torch.device(\"cpu\")):\n",
    "    # 1) Copy the global model locally (each client trains locally starting from the same weights)\n",
    "    model = TwoNN().to(device)\n",
    "    model.load_state_dict({k: v.detach().clone() for k, v in global_model.state_dict().items()})\n",
    "    model.train()  # Switch to training mode (enable Dropout/BN, etc.)\n",
    "\n",
    "    # 2) Define loss function and optimizer (paper uses SGD; simplified here)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # 3) Build the client’s DataLoader (Subset = local client dataset; B=None means full batch)\n",
    "    subset = Subset(dataset, indices.tolist())\n",
    "    if B is None:\n",
    "        loader = DataLoader(subset, batch_size=len(subset), shuffle=False)   # Full batch GD\n",
    "    else:\n",
    "        loader = DataLoader(subset, batch_size=B, shuffle=True, drop_last=False)\n",
    "\n",
    "    # 4) Local training for E epochs (multiple local steps before communication)\n",
    "    for _ in range(E):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()           # Reset gradients\n",
    "            logits = model(x)               # Forward pass\n",
    "            loss = criterion(logits, y)     # Compute cross-entropy loss\n",
    "            loss.backward()                 # Backward pass\n",
    "            optimizer.step()                # Update parameters\n",
    "\n",
    "    # 5) Return updated local model parameters and the number of samples (for weighted aggregation)\n",
    "    return model.state_dict(), len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44fb2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg_aggregate(updates):\n",
    "    total = sum(n_k for _, n_k in updates)   # Total number of samples n = sum(n_k)\n",
    "    keys = list(updates[0][0].keys())        # Get all parameter keys (e.g., 'fc1.weight')\n",
    "    agg = {k: torch.zeros_like(updates[0][0][k]) for k in keys}  # Initialize with zeros\n",
    "\n",
    "    for state_k, n_k in updates:             # Iterate over each client’s parameters\n",
    "        w = n_k / total                      # Weight = client sample size / total samples\n",
    "        for k in keys:\n",
    "            agg[k] += state_k[k] * w         # Weighted accumulation -> weighted average\n",
    "\n",
    "    return agg                               # Return the new global parameter dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcfe1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, test_ds, device=torch.device(\"cpu\")):\n",
    "    model.eval()   # Evaluation mode (disable Dropout/BN training behavior)\n",
    "    loader = DataLoader(test_ds, batch_size=1024, shuffle=False)  # Large batch for faster testing\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(dim=1)        # Prediction = class with highest probability\n",
    "        correct += (pred == y).sum().item()  # Count correct predictions\n",
    "        total += y.size(0)                   # Count total samples\n",
    "\n",
    "    return correct / total                   # Return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "968d1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once_with_lr(\n",
    "    lr: float,\n",
    "    *,\n",
    "    seed: int = 2025,\n",
    "    partition: str = \"noniid\",   # Data partitioning scheme: \"iid\" or \"noniid\"\n",
    "    K: int = 100,                # Total number of clients\n",
    "    C: float = 0.1,              # Fraction of clients participating in each round\n",
    "    E: int = 1,                  # Local training epochs per client\n",
    "    B: int | None = 10,          # Local mini-batch size (None = full batch)\n",
    "    target_acc: float = 0.97,    # Target accuracy threshold to stop training\n",
    "    max_rounds: int = 500,      \n",
    "    device: torch.device = torch.device(\"cpu\"), \n",
    "    DATA_DIR: str | None = None,\n",
    ") -> int | None:\n",
    "    \"\"\"Run one FedAvg training with given (seed, lr).\n",
    "    Returns the round number when target_acc is first achieved,\n",
    "    or None if not reached within max_rounds.\n",
    "    \"\"\"\n",
    "    set_seed(seed)  # Ensure reproducibility for random operations\n",
    "\n",
    "    # ---- Dataset preparation ----\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),                      # Convert images (PIL/ndarray) to tensors scaled in [0,1]\n",
    "        transforms.Normalize((0.1307,), (0.3081,)), # Normalize MNIST with mean=0.1307 and std=0.3081\n",
    "    ])\n",
    "    DATA_DIR = os.path.expanduser(\"~/datasets/mnist\")  # Directory to store MNIST dataset\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)               # Create directory if not exists\n",
    "    train_ds = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transform)   # Training dataset\n",
    "    test_ds  = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=transform)  # Test dataset\n",
    "\n",
    "    # ---- Partition data among clients ----\n",
    "    if partition == \"iid\":\n",
    "        parts = make_iid_parts(train_ds, K=K)                 # Split dataset into IID partitions\n",
    "    else:\n",
    "        parts = make_pathological_noniid_parts(train_ds, K=K) # Split dataset into pathological non-IID partitions\n",
    "\n",
    "    # ---- Initialize global model ----\n",
    "    global_model = TwoNN().to(device)  # Two-layer NN model for MNIST, moved to device\n",
    "\n",
    "    # ---- Federated learning loop ----\n",
    "    best_acc = 0.0          # Track the best test accuracy observed so far\n",
    "    rounds_to_target = None # Record the first round achieving target accuracy\n",
    "\n",
    "    for t in range(1, max_rounds + 1):  # Iterate over communication rounds\n",
    "        m = max(int(C * K), 1)  # Number of clients selected this round (at least 1)\n",
    "        selected = np.random.choice(np.arange(K), size=m, replace=False)  # Randomly select m clients\n",
    "\n",
    "        updates = []  # List to store client updates (model state, sample count)\n",
    "        for k in selected:\n",
    "            state_k, n_k = client_update(\n",
    "                global_model, train_ds, parts[k],  # Client trains on its local partition\n",
    "                E=E, B=B, lr=lr, device=device     # Using given hyperparameters\n",
    "            )\n",
    "            updates.append((state_k, n_k))  # Collect local update and dataset size\n",
    "\n",
    "        new_state = fedavg_aggregate(updates)  # Aggregate client updates (weighted average)\n",
    "        global_model.load_state_dict(new_state)  # Update global model with aggregated state\n",
    "\n",
    "        acc = evaluate(global_model, test_ds, device)  # Evaluate global model on test set\n",
    "        best_acc = max(best_acc, acc)                  # Track best accuracy seen so far\n",
    "\n",
    "        if best_acc >= target_acc and rounds_to_target is None:\n",
    "            rounds_to_target = t  # Record the round when target accuracy is first reached\n",
    "            break                 # Stop early since target is achieved\n",
    "\n",
    "    return rounds_to_target  # Return round number, or None if not achieved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9382664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grid] partition=noniid, K=100, C=0.1, E=1, B=10, target=97%, max_rounds=500, seed=2025\n",
      "------------------------------------------------------------------------\n",
      "lr=0.03   | rounds=359\n",
      "lr=0.05   | rounds=227\n",
      "lr=0.07   | rounds=190\n",
      "lr=0.1    | rounds=142\n",
      "lr=0.15   | rounds=145\n",
      "------------------------------------------------------------------------\n",
      "BEST lr=0.1 | rounds-to-97% = 142\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # ---- Fixed experiment knobs (same as paper defaults) ----\n",
    "    partition = \"noniid\"              # \"iid\" or \"noniid\"\n",
    "    K, C, E, B = 100, 0.1, 1, 10\n",
    "    target_acc = 0.97\n",
    "    max_rounds = 500\n",
    "    device = torch.device(\"cpu\")\n",
    "    DATA_DIR = os.path.expanduser(\"~/datasets/mnist\")\n",
    "    seed = 2025                       \n",
    "\n",
    "    # ---- Learning-rate candidates (you can tweak/expand) ----\n",
    "    lr_candidates = [0.03, 0.05, 0.07, 0.1, 0.15]\n",
    "\n",
    "    print(f\"[Grid] partition={partition}, K={K}, C={C}, E={E}, B={B}, \"\n",
    "          f\"target={int(target_acc*100)}%, max_rounds={max_rounds}, seed={seed}\")\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "    results: list[tuple[float, int | None]] = []\n",
    "    for lr in lr_candidates:\n",
    "        r = run_once_with_lr(\n",
    "            lr,\n",
    "            seed=seed,\n",
    "            partition=partition, K=K, C=C, E=E, B=B,\n",
    "            target_acc=target_acc, max_rounds=max_rounds,\n",
    "            device=device, DATA_DIR=DATA_DIR,\n",
    "        )\n",
    "        results.append((lr, r))\n",
    "        if r is None:\n",
    "            print(f\"lr={lr:<6g} | rounds=NA  (did not reach {int(target_acc*100)}%)\")\n",
    "        else:\n",
    "            print(f\"lr={lr:<6g} | rounds={r}\")\n",
    "\n",
    "    # ---- pick best: minimal rounds among those that reached target ----\n",
    "    reached = [(lr, r) for (lr, r) in results if r is not None]\n",
    "    print(\"-\" * 72)\n",
    "    if reached:\n",
    "        best_lr, best_rounds = min(reached, key=lambda x: x[1])\n",
    "        print(f\"BEST lr={best_lr} | rounds-to-{int(target_acc*100)}% = {best_rounds}\")\n",
    "    else:\n",
    "        print(f\"No lr reached {int(target_acc*100)}% within {max_rounds} rounds. \"\n",
    "              f\"Try increasing max_rounds or widening lr grid.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
